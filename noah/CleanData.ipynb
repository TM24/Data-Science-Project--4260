{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook Cleans our Data\n",
    "\n",
    "### This is quite a lot of data so it may take some time to complete depending on hardware resources\n",
    "\n",
    "### I would recommend only having this notbook open and closing all other applications when running, unless you have good specs on desktop\n",
    "\n",
    "#### Feel free to change the values to whatever you want if you are experementing with removing other data, like _std, _5_last_games, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load and clean data\n",
    "# We remove Standard deviation columns, and season average, and last 5 match sum statistics\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"Load a dataset and apply cleaning steps.\n",
    "    Removes STD, season average, and last 5 matched sum\n",
    "    Fills NULL/nan values with 0/zero\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        df = df.drop(df.filter(regex='_std$|_season_average$|5_last_match_sum$').columns, axis=1)\n",
    "\n",
    "        # Fill remaining missing values with zero\n",
    "        df = df.fillna(0)\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to reshape player data (convert multiple rows per ID into a single row)\n",
    "def reshape_player_data(player_df):\n",
    "    \"\"\"Convert player stats from multiple rows to a single row per ID.\"\"\"\n",
    "    if player_df is None:\n",
    "        return None\n",
    "\n",
    "    # Add a unique number to each player's stats for a given ID\n",
    "    player_df[\"player_number\"] = player_df.groupby(\"ID\").cumcount() + 1\n",
    "\n",
    "    # Reshape using pivot_table (each player gets a numbered column)\n",
    "    player_df = player_df.pivot(index=\"ID\", columns=\"player_number\")\n",
    "    \n",
    "    # Flatten MultiIndex columns\n",
    "    player_df.columns = [f\"{col[0]}_P{col[1]}\" for col in player_df.columns]\n",
    "    \n",
    "    # Reset index so ID is a column again\n",
    "    player_df = player_df.reset_index()\n",
    "    \n",
    "    return player_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load and clean data\n",
    "# further cleaning the data after reshaping,\n",
    "# removing Players 6-27, and TEAM_NAME, and Player_name, and League\n",
    "def clean_data_V2(df):\n",
    "    \"\"\"Load a dataset and apply cleaning steps.\n",
    "    remove P6-27\n",
    "    and POSITION\"\"\"\n",
    "    try:\n",
    "        df = df.drop(df.filter(regex='P2[0-7]$|^POSITION|P1[0-9]$|^TEAM_NAME|^LEAGUE|^PLAYER_NAME|P[6-9]$').columns, axis=1)\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading {df}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to merge team and reshaped player data\n",
    "# this function calls the other functions to clean the data\n",
    "def merge_team_and_players(team_path, player_path, output_path):\n",
    "    \"\"\"Merge a team dataset with its players into a single row per ID and save to file.\"\"\"\n",
    "    print(f\"Processing and saving: {output_path}\")\n",
    "\n",
    "    # Load and clean data\n",
    "    team_df = load_and_clean_data(team_path)\n",
    "    player_df = load_and_clean_data(player_path)\n",
    "\n",
    "    if team_df is None or player_df is None:\n",
    "        print(f\"Skipping {output_path} due to missing data.\")\n",
    "        return\n",
    "\n",
    "    # Reshape player stats\n",
    "    reshaped_players = reshape_player_data(player_df)\n",
    "\n",
    "    # Merge team stats with reshaped player stats (1 row per ID)\n",
    "    merged_df = pd.merge(team_df, reshaped_players, on='ID', how='left')\n",
    "    \n",
    "    cleaned_v2 = clean_data_V2(merged_df)\n",
    "\n",
    "    # Save final dataset\n",
    "    cleaned_v2.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {output_path} ({cleaned_v2.shape[0]} rows, {cleaned_v2.shape[1]} columns)\")\n",
    "\n",
    "    # Free up memory\n",
    "    del team_df, player_df, reshaped_players, merged_df, cleaned_v2\n",
    "    gc.collect()\n",
    "    return None\n",
    "    # return team_df, reshaped_players, cleaned_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "# Put the path to your UNCLEAN data\n",
    "data_paths = {\n",
    "    \"train_home_team\": \"C:/Path/To/Data/Train_Data/train_home_team_statistics_df.csv\",\n",
    "    \"train_home_player\": \"C:/Path/To/Data/Train_Data/train_home_player_statistics_df.csv\",\n",
    "    \"train_away_team\": \"C:/Path/To/Data/Train_Data/train_away_team_statistics_df.csv\",\n",
    "    \"train_away_player\": \"C:/Path/To/Data/Train_Data/train_away_player_statistics_df.csv\",\n",
    "    \"test_home_team\": \"C:/Path/To/Data/Test_Data/test_home_team_statistics_df.csv\",\n",
    "    \"test_home_player\": \"C:/Path/To/Data/Test_Data/test_home_player_statistics_df.csv\",\n",
    "    \"test_away_team\": \"C:/Path/To/Data/Test_Data/test_away_team_statistics_df.csv\",\n",
    "    \"test_away_player\": \"C:/Path/To/Data/Test_Data/test_away_player_statistics_df.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CLeaning Columns\n",
    "# Create output directory\n",
    "output_dir = \"C:/Path/To/Data/cleanedData/TestingCleanedDataGroupTest\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Merge and save Home datasets separately\n",
    "# teamDF, reshapedDATA, cLEANV2 = merge_team_and_players(data_paths[\"train_home_team\"], data_paths[\"train_home_player\"], os.path.join(output_dir, \"train_merged_home.csv\"))\n",
    "merge_team_and_players(data_paths[\"train_home_team\"], data_paths[\"train_home_player\"], os.path.join(output_dir, \"train_home.csv\"))\n",
    "merge_team_and_players(data_paths[\"test_home_team\"], data_paths[\"test_home_player\"], os.path.join(output_dir, \"test_home.csv\"))\n",
    "\n",
    "# # Merge and save Away datasets separately\n",
    "merge_team_and_players(data_paths[\"train_away_team\"], data_paths[\"train_away_player\"], os.path.join(output_dir, \"train_away.csv\"))\n",
    "merge_team_and_players(data_paths[\"test_away_team\"], data_paths[\"test_away_player\"], os.path.join(output_dir, \"test_away.csv\"))\n",
    "\n",
    "print(\"All datasets merged and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Looking at the size diference at different stages of cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(teamDF.shape)\n",
    "# print(teamDF.head)\n",
    "print(reshapedDATA.shape)\n",
    "# print(reshapedDATA.head)\n",
    "print(cLEANV2.shape)\n",
    "# print(cLEANV2.head)\n",
    "print(cLEANV2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Away and Home data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_home = pd.read_csv('C:/Path/You?Want?Data/To/train_home.csv')\n",
    "X_train_away = pd.read_csv('C:/Path/You?Want?Data/To/train_away.csv')\n",
    "\n",
    "X_test_home = pd.read_csv('C:/Path/You?Want?Data/To/test_home.csv')\n",
    "X_test_away = pd.read_csv('C:/Path/You?Want?Data/To/test_away.csv')\n",
    "\n",
    "X_train_combined = pd.concat([X_train_home, X_train_away], axis=1)\n",
    "X_test_combined = pd.concat([X_test_home, X_test_away], axis=1)\n",
    "\n",
    "\n",
    "X_train_combined.to_csv(f\"C:/Path/You?Want?Data/To/trainV5.1_combine.csv\", index=False)\n",
    "X_test_combined.to_csv(f\"C:/Path/You?Want?Data/To/testV5.1_combine.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Longer Needed\n",
    "\n",
    "\n",
    "### Ensuring that Y and X have same ID values\n",
    "\n",
    "In the function below, Give the path to your cleaned data. \n",
    "Just use your output paths from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Y_data = \"Path/to/cleaned/data\"\n",
    "home =\"Path/to/cleaned/data\"\n",
    "away = \"Path/to/cleaned/data\"\n",
    "\n",
    "\n",
    "def align_training_datasets(Y_train, X_train_home, X_train_away, output_path_prefix):\n",
    "    \"\"\"\n",
    "    Align Y_train, X_train_home, and X_train_away datasets to ensure they have the same IDs\n",
    "    and number of rows. Also checks for duplicates and validates data integrity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Y_train : pandas.DataFrame\n",
    "        Training labels dataset containing IDs\n",
    "    X_train_home : pandas.DataFrame\n",
    "        Home team training features dataset containing IDs\n",
    "    X_train_away : pandas.DataFrame\n",
    "        Away team training features dataset containing IDs\n",
    "    output_path_prefix : str\n",
    "        Prefix for the output files. Will append _y.csv, _home.csv, and _away.csv\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (aligned_y_train, aligned_x_train_home, aligned_x_train_away)\n",
    "    \"\"\"\n",
    "    # Convert IDs to string type for consistent comparison\n",
    "    Y_train['ID'] = Y_train['ID'].astype(str)\n",
    "    X_train_home['ID'] = X_train_home['ID'].astype(str)\n",
    "    X_train_away['ID'] = X_train_away['ID'].astype(str)\n",
    "    \n",
    "    # Check for duplicate IDs in each dataset\n",
    "    print(\"Checking for duplicates:\")\n",
    "    print(f\"Y_train duplicates: {Y_train['ID'].duplicated().sum()}\")\n",
    "    print(f\"X_train_home duplicates: {X_train_home['ID'].duplicated().sum()}\")\n",
    "    print(f\"X_train_away duplicates: {X_train_away['ID'].duplicated().sum()}\")\n",
    "    \n",
    "    # Get sets of IDs from each dataset\n",
    "    y_ids = set(Y_train['ID'])\n",
    "    home_ids = set(X_train_home['ID'])\n",
    "    away_ids = set(X_train_away['ID'])\n",
    "    \n",
    "    # Find common IDs across all datasets\n",
    "    common_ids = y_ids.intersection(home_ids).intersection(away_ids)\n",
    "    \n",
    "    # Print analysis of missing IDs\n",
    "    print(\"/nMissing ID Analysis:\")\n",
    "    print(f\"IDs in Y_train but not in X_train_home: {y_ids - home_ids}\")\n",
    "    print(f\"IDs in Y_train but not in X_train_away: {y_ids - away_ids}\")\n",
    "    print(f\"IDs in X_train_home but not in Y_train: {home_ids - y_ids}\")\n",
    "    print(f\"IDs in X_train_away but not in Y_train: {away_ids - y_ids}\")\n",
    "    \n",
    "    # Filter all datasets to keep only common IDs\n",
    "    aligned_y = Y_train[Y_train['ID'].isin(common_ids)].copy()\n",
    "    aligned_home = X_train_home[X_train_home['ID'].isin(common_ids)].copy()\n",
    "    aligned_away = X_train_away[X_train_away['ID'].isin(common_ids)].copy()\n",
    "    \n",
    "    # Sort all datasets by ID to ensure matching order\n",
    "    aligned_y = aligned_y.sort_values('ID').reset_index(drop=True)\n",
    "    aligned_home = aligned_home.sort_values('ID').reset_index(drop=True)\n",
    "    aligned_away = aligned_away.sort_values('ID').reset_index(drop=True)\n",
    "    \n",
    "    # Print shape information\n",
    "    print(\"/nDataset Shapes:\")\n",
    "    print(f\"Original Y_train shape: {Y_train.shape}\")\n",
    "    print(f\"Original X_train_home shape: {X_train_home.shape}\")\n",
    "    print(f\"Original X_train_away shape: {X_train_away.shape}\")\n",
    "    print(f\"/nAligned Y_train shape: {aligned_y.shape}\")\n",
    "    print(f\"Aligned X_train_home shape: {aligned_home.shape}\")\n",
    "    print(f\"Aligned X_train_away shape: {aligned_away.shape}\")\n",
    "    \n",
    "    # Verify alignment\n",
    "    all_aligned = (\n",
    "        aligned_y.shape[0] == aligned_home.shape[0] == aligned_away.shape[0] and\n",
    "        (aligned_y['ID'] == aligned_home['ID']).all() and\n",
    "        (aligned_y['ID'] == aligned_away['ID']).all()\n",
    "    )\n",
    "    print(f\"/nDatasets properly aligned: {all_aligned}\")\n",
    "    \n",
    "    # Save aligned datasets\n",
    "    if output_path_prefix:\n",
    "        aligned_y.to_csv(f\"{output_path_prefix}_y.csv\", index=False)\n",
    "        aligned_home.to_csv(f\"{output_path_prefix}_home.csv\", index=False)\n",
    "        aligned_away.to_csv(f\"{output_path_prefix}_away.csv\", index=False)\n",
    "        print(f\"/nAligned datasets saved with prefix: {output_path_prefix}\")\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "align_training_datasets(Y_data, home, away, 'C:/Path/To/Data/Test_Data/DataV4.1_test')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
